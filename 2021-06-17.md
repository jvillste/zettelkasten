# how to represent nulls in indexes?
# timestamps as offsets
A data segment could define a custom timestamp column type by specifying a starting year and unit: year, month, day, hour, second, millisecond. A timestamp would then be a number of such units from the given year.

A [[Variable length quantity]] should be used as the offset to avoid running out of available range at any point in the future.

A date for example would then be in most applications only two bytes because that would cover a  2^14/365=44 year time span.

A default year would be prefered to make the timestamps in different data segments be comparable without adding an offset. Choosing 1970 as the default would make the timestamps compatible with the unix epoch. That would make dates three bytes long until the year 1970+2^21/365=7715. 